from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
import re


class LLMParser:
    def __init__(self, model_path="01-ai/Yi-1.5-9B-Chat", device="auto"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=device,
            attn_implementation="eager"
        )

    def build_prompt(self, user_input: str) -> str:
        """æ„é€ æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªå®å¯æ¢¦å›¾é‰´åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæå–æ„å›¾å’Œç›®æ ‡å®ä½“ã€‚

ã€ä»»åŠ¡è¯´æ˜ã€‘
è¯·ä¸¥æ ¼æå–ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
1. targetï¼šç”¨æˆ·æåˆ°çš„å®ä½“åç§°ï¼ˆå¦‚â€œçš®å¡ä¸˜â€ã€â€œæ¶è‡­â€ã€â€œåä¸‡ä¼ç‰¹â€ï¼‰
2. target_typeï¼šè¯¥å®ä½“çš„ç±»å‹ï¼ˆå®å¯æ¢¦ / æŠ€èƒ½ / ç‰¹æ€§ï¼‰
3. intentï¼šç”¨æˆ·æƒ³äº†è§£çš„å†…å®¹ï¼ˆå¦‚ æŸ¥è¯¢ã€è¿›åŒ–ã€å±æ€§ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»…è¾“å‡ºä¸€è¡Œ JSONï¼Œä¸è¦æ·»åŠ è§£é‡Šï¼š
{{"target": "å®ä½“åç§°", "target_type": "å®å¯æ¢¦/æŠ€èƒ½/ç‰¹æ€§", "intent": "æ„å›¾"}}

ã€ç¤ºä¾‹ã€‘
ç”¨æˆ·ï¼šçš®å¡ä¸˜æ˜¯æŠ€èƒ½è¿˜æ˜¯å®å¯æ¢¦ï¼Ÿ
åŠ©æ‰‹ï¼š{{"target": "çš®å¡ä¸˜", "target_type": "å®å¯æ¢¦", "intent": "æŸ¥è¯¢"}}

ç”¨æˆ·ï¼š{user_input}
åŠ©æ‰‹ï¼š"""

    def extract_intent_entities(self, user_input: str) -> dict:
        """æ‰§è¡Œæ„å›¾è¯†åˆ«ä»»åŠ¡"""
        prompt = self.build_prompt(user_input)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        try:
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=False,
                temperature=0.1,
                eos_token_id=self.tokenizer.eos_token_id
            )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"[DEBUG] æ¨¡å‹åŸå§‹è¾“å‡º:\n{response}")

            # å°è¯•åŒ¹é… JSON æ ¼å¼
            match = re.search(r'\{.*?\}', response)
            if match:
                data = json.loads(match.group(0))
                return {
                    "target": data.get("target", "æœªçŸ¥"),
                    "target_type": data.get("target_type", "æœªçŸ¥"),
                    "intent": data.get("intent", "æœªçŸ¥")
                }

        except Exception as e:
            print(f"[ERROR] æ¨¡å‹è¾“å‡ºæˆ–è§£æå¤±è´¥: {e}")

        # fallback è¿”å›
        return {
            "target": "æœªçŸ¥",
            "target_type": "æœªçŸ¥",
            "intent": "æœªçŸ¥"
        }


# æµ‹è¯•ç”¨
if __name__ == "__main__":
    parser = LLMParser()
    print("ğŸ” LLM æ„å›¾è¯†åˆ«æµ‹è¯•ï¼ˆè¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ï¼‰\nè¾“å…¥ 'exit' é€€å‡ºã€‚")

    while True:
        query = input("\nğŸ’¬ è¯·è¾“å…¥é—®é¢˜ï¼š")
        if query.strip().lower() == "exit":
            break

        result = parser.extract_intent_entities(query)
        print("ğŸ“¤ æå–ç»“æœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
